{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.stem\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from src.SemEvalData import SemEvalData\n",
    "from src.JigsawData import JigsawData\n",
    "from src.preprocessing import preprocess_bayes, getSpansByToxicWords, getToxicWordsBayes\n",
    "import joblib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.datasets\n",
    "import sklearn.feature_extraction.text\n",
    "import sklearn.naive_bayes\n",
    "import sklearn.metrics\n",
    "import sklearn.model_selection\n",
    "import sklearn.pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from test_sentence import test_bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/patrycja/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stemmer = nltk.stem.SnowballStemmer('english')\n",
    "lemmer = nltk.stem.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load data\n",
    "train_data_semeval = SemEvalData()\n",
    "train_data_semeval.load_data(\"data/tsd_train.csv\")\n",
    "train_df_preprocessed = train_data_semeval.preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>spans</th>\n",
       "      <th>text</th>\n",
       "      <th>toxicity</th>\n",
       "      <th>toxic_words</th>\n",
       "      <th>original_text</th>\n",
       "      <th>sentences</th>\n",
       "      <th>toxicity_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19,...</td>\n",
       "      <td>another violent and aggressive immigrant killi...</td>\n",
       "      <td>1</td>\n",
       "      <td>[violent and aggressive immigrant]</td>\n",
       "      <td>Another violent and aggressive immigrant killi...</td>\n",
       "      <td>[another violent and aggressive immigrant kill...</td>\n",
       "      <td>[1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[33, 34, 35, 36, 37, 38, 39]</td>\n",
       "      <td>i am 56 years old, i am not your fucking junio...</td>\n",
       "      <td>1</td>\n",
       "      <td>[fucking]</td>\n",
       "      <td>I am 56 years old, I am not your fucking junio...</td>\n",
       "      <td>[i am 56 years old, i am not your fucking juni...</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0, 1, 2, 3]</td>\n",
       "      <td>damn, a whole family. sad indeed.</td>\n",
       "      <td>1</td>\n",
       "      <td>[damn]</td>\n",
       "      <td>Damn, a whole family. Sad indeed.</td>\n",
       "      <td>[damn, a whole family., sad indeed.]</td>\n",
       "      <td>[1.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17]</td>\n",
       "      <td>what a knucklehead. how can anyone not know th...</td>\n",
       "      <td>1</td>\n",
       "      <td>[knucklehead]</td>\n",
       "      <td>What a knucklehead. How can anyone not know th...</td>\n",
       "      <td>[what a knucklehead., how can anyone not know ...</td>\n",
       "      <td>[1.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[32, 33, 34, 35, 36, 37, 38]</td>\n",
       "      <td>who do you think should do the killing?  anyon...</td>\n",
       "      <td>1</td>\n",
       "      <td>[killing]</td>\n",
       "      <td>\"who do you think should do the killing?\"\\n\\nA...</td>\n",
       "      <td>[who do you think should do the killing?, anyo...</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[]</td>\n",
       "      <td>but, but, but, is not a defense.  its not even...</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>But, but, but, is NOT a defense.  It's not eve...</td>\n",
       "      <td>[but, but, but, is not a defense., its not eve...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[39, 40, 41, 42, 43, 44, 46, 47, 48, 49, 50, 5...</td>\n",
       "      <td>please people, stop using these silly, stupid ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[stupid, emoticons]</td>\n",
       "      <td>Please people, stop using these silly, stupid ...</td>\n",
       "      <td>[please people, stop using these silly, stupid...</td>\n",
       "      <td>[1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[0, 1, 2, 3]</td>\n",
       "      <td>dumb.</td>\n",
       "      <td>1</td>\n",
       "      <td>[dumb]</td>\n",
       "      <td>Dumb.</td>\n",
       "      <td>[dumb.]</td>\n",
       "      <td>[1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[49, 50, 51, 52, 53, 54, 147, 148, 149, 150, 1...</td>\n",
       "      <td>obamacare is on its last gasping breaths.   yo...</td>\n",
       "      <td>1</td>\n",
       "      <td>[idiots, stupid]</td>\n",
       "      <td>Obamacare is on it's last gasping breaths.   Y...</td>\n",
       "      <td>[obamacare is on its last gasping breaths., yo...</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[32, 33, 34, 35, 36, 37, 38, 39]</td>\n",
       "      <td>crooked trump = guilty as hell. pathetic</td>\n",
       "      <td>1</td>\n",
       "      <td>[pathetic]</td>\n",
       "      <td>CROOKED Trump = GUILTY as hell.\\npathetic</td>\n",
       "      <td>[crooked trump = guilty as hell., pathetic]</td>\n",
       "      <td>[0.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[223, 224, 225, 226, 227, 228, 229, 230]</td>\n",
       "      <td>you conveniently dont mention your cohorts who...</td>\n",
       "      <td>1</td>\n",
       "      <td>[imbecile]</td>\n",
       "      <td>You conveniently don't mention your cohorts wh...</td>\n",
       "      <td>[you conveniently dont mention your cohorts wh...</td>\n",
       "      <td>[0.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[187, 188, 189, 190, 191]</td>\n",
       "      <td>brilliant retort... it only makes sense that i...</td>\n",
       "      <td>1</td>\n",
       "      <td>[idiot]</td>\n",
       "      <td>Brilliant retort... It only makes sense that i...</td>\n",
       "      <td>[brilliant retort... it only makes sense that ...</td>\n",
       "      <td>[0.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[78, 79, 80, 81, 82, 83, 84, 85, 86]</td>\n",
       "      <td>too bad.  jeff sessions has no common sense an...</td>\n",
       "      <td>1</td>\n",
       "      <td>[repugnant]</td>\n",
       "      <td>Too bad.  Jeff Sessions has no common sense an...</td>\n",
       "      <td>[too bad., jeff sessions has no common sense a...</td>\n",
       "      <td>[0.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[38, 39, 40, 41]</td>\n",
       "      <td>this man is a sickening, mentally ill joke.</td>\n",
       "      <td>1</td>\n",
       "      <td>[joke]</td>\n",
       "      <td>This man is a sickening, mentally ill joke.</td>\n",
       "      <td>[this man is a sickening, mentally ill joke.]</td>\n",
       "      <td>[1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[193, 194, 195, 196, 197, 198, 199]</td>\n",
       "      <td>religion is a crutch for the ignorant masses o...</td>\n",
       "      <td>1</td>\n",
       "      <td>[protest]</td>\n",
       "      <td>Religion is a crutch for the ignorant masses o...</td>\n",
       "      <td>[religion is a crutch for the ignorant masses ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[164, 165, 166, 167, 168, 169, 170, 171]</td>\n",
       "      <td>no, im saying, based on previous comments youv...</td>\n",
       "      <td>1</td>\n",
       "      <td>[f*** you]</td>\n",
       "      <td>No, I'm saying, based on previous comments you...</td>\n",
       "      <td>[no, im saying, based on previous comments you...</td>\n",
       "      <td>[0.0, 0.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[87, 88, 89, 90, 91, 92]</td>\n",
       "      <td>in doing so, they cite oil industry assurances...</td>\n",
       "      <td>1</td>\n",
       "      <td>[stupid]</td>\n",
       "      <td>\"In doing so, they cite oil industry assurance...</td>\n",
       "      <td>[in doing so, they cite oil industry assurance...</td>\n",
       "      <td>[1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 23, ...</td>\n",
       "      <td>what a sociopathic and parasitic leader we have.</td>\n",
       "      <td>1</td>\n",
       "      <td>[sociopathic, parasitic]</td>\n",
       "      <td>What a sociopathic and parasitic leader we have.</td>\n",
       "      <td>[what a sociopathic and parasitic leader we ha...</td>\n",
       "      <td>[1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 199, ...</td>\n",
       "      <td>we would all be better off if the president wa...</td>\n",
       "      <td>1</td>\n",
       "      <td>[sycophants, bum]</td>\n",
       "      <td>We would all be better off if the President wa...</td>\n",
       "      <td>[we would all be better off if the president w...</td>\n",
       "      <td>[0.0, 0.0, 1.0, 0.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[12, 13, 14, 15]</td>\n",
       "      <td>trump is so dumb! hahahahahaha!</td>\n",
       "      <td>1</td>\n",
       "      <td>[dumb]</td>\n",
       "      <td>Trump is so dumb!\\nHAHAHAHAHAHA!</td>\n",
       "      <td>[trump is so dumb!, hahahahahaha!]</td>\n",
       "      <td>[1.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[0, 1, 2, 3, 50, 51, 52, 53, 54, 55, 56, 57, 5...</td>\n",
       "      <td>kill some more kids and then complain about gu...</td>\n",
       "      <td>1</td>\n",
       "      <td>[kill, lol the left is a joke]</td>\n",
       "      <td>Kill some more kids and then complain about gu...</td>\n",
       "      <td>[kill some more kids and then complain about g...</td>\n",
       "      <td>[1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[]</td>\n",
       "      <td>jim  i think donald is a con man and a dangero...</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>Jim\\n\\nI think Donald is a con man and a dange...</td>\n",
       "      <td>[jim  i think donald is a con man and a danger...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[99, 100, 101, 102]</td>\n",
       "      <td>theres plenty of wrong-wingers out there that ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[fool]</td>\n",
       "      <td>There's plenty of wrong-wingers out there that...</td>\n",
       "      <td>[theres plenty of wrong-wingers out there that...</td>\n",
       "      <td>[0.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[217, 218, 219, 220, 221, 222, 223, 224]</td>\n",
       "      <td>excellent point!  what if a mob assembled outs...</td>\n",
       "      <td>1</td>\n",
       "      <td>[ignorant]</td>\n",
       "      <td>Excellent point!  What if a mob assembled outs...</td>\n",
       "      <td>[excellent point!, what if a mob assembled out...</td>\n",
       "      <td>[0.0, 0.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[]</td>\n",
       "      <td>exposing hypocrites like trump and pence is th...</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>Exposing hypocrites like Trump and Pence is th...</td>\n",
       "      <td>[exposing hypocrites like trump and pence is t...</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                spans  \\\n",
       "0   [8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19,...   \n",
       "1                        [33, 34, 35, 36, 37, 38, 39]   \n",
       "2                                        [0, 1, 2, 3]   \n",
       "3           [7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17]   \n",
       "4                        [32, 33, 34, 35, 36, 37, 38]   \n",
       "5                                                  []   \n",
       "6   [39, 40, 41, 42, 43, 44, 46, 47, 48, 49, 50, 5...   \n",
       "7                                        [0, 1, 2, 3]   \n",
       "8   [49, 50, 51, 52, 53, 54, 147, 148, 149, 150, 1...   \n",
       "9                    [32, 33, 34, 35, 36, 37, 38, 39]   \n",
       "10           [223, 224, 225, 226, 227, 228, 229, 230]   \n",
       "11                          [187, 188, 189, 190, 191]   \n",
       "12               [78, 79, 80, 81, 82, 83, 84, 85, 86]   \n",
       "13                                   [38, 39, 40, 41]   \n",
       "14                [193, 194, 195, 196, 197, 198, 199]   \n",
       "15           [164, 165, 166, 167, 168, 169, 170, 171]   \n",
       "16                           [87, 88, 89, 90, 91, 92]   \n",
       "17  [7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 23, ...   \n",
       "18  [86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 199, ...   \n",
       "19                                   [12, 13, 14, 15]   \n",
       "20  [0, 1, 2, 3, 50, 51, 52, 53, 54, 55, 56, 57, 5...   \n",
       "21                                                 []   \n",
       "22                                [99, 100, 101, 102]   \n",
       "23           [217, 218, 219, 220, 221, 222, 223, 224]   \n",
       "24                                                 []   \n",
       "\n",
       "                                                 text  toxicity  \\\n",
       "0   another violent and aggressive immigrant killi...         1   \n",
       "1   i am 56 years old, i am not your fucking junio...         1   \n",
       "2                   damn, a whole family. sad indeed.         1   \n",
       "3   what a knucklehead. how can anyone not know th...         1   \n",
       "4   who do you think should do the killing?  anyon...         1   \n",
       "5   but, but, but, is not a defense.  its not even...         0   \n",
       "6   please people, stop using these silly, stupid ...         1   \n",
       "7                                               dumb.         1   \n",
       "8   obamacare is on its last gasping breaths.   yo...         1   \n",
       "9            crooked trump = guilty as hell. pathetic         1   \n",
       "10  you conveniently dont mention your cohorts who...         1   \n",
       "11  brilliant retort... it only makes sense that i...         1   \n",
       "12  too bad.  jeff sessions has no common sense an...         1   \n",
       "13        this man is a sickening, mentally ill joke.         1   \n",
       "14  religion is a crutch for the ignorant masses o...         1   \n",
       "15  no, im saying, based on previous comments youv...         1   \n",
       "16  in doing so, they cite oil industry assurances...         1   \n",
       "17   what a sociopathic and parasitic leader we have.         1   \n",
       "18  we would all be better off if the president wa...         1   \n",
       "19                    trump is so dumb! hahahahahaha!         1   \n",
       "20  kill some more kids and then complain about gu...         1   \n",
       "21  jim  i think donald is a con man and a dangero...         0   \n",
       "22  theres plenty of wrong-wingers out there that ...         1   \n",
       "23  excellent point!  what if a mob assembled outs...         1   \n",
       "24  exposing hypocrites like trump and pence is th...         0   \n",
       "\n",
       "                           toxic_words  \\\n",
       "0   [violent and aggressive immigrant]   \n",
       "1                            [fucking]   \n",
       "2                               [damn]   \n",
       "3                        [knucklehead]   \n",
       "4                            [killing]   \n",
       "5                                   []   \n",
       "6                  [stupid, emoticons]   \n",
       "7                               [dumb]   \n",
       "8                     [idiots, stupid]   \n",
       "9                           [pathetic]   \n",
       "10                          [imbecile]   \n",
       "11                             [idiot]   \n",
       "12                         [repugnant]   \n",
       "13                              [joke]   \n",
       "14                           [protest]   \n",
       "15                          [f*** you]   \n",
       "16                            [stupid]   \n",
       "17            [sociopathic, parasitic]   \n",
       "18                   [sycophants, bum]   \n",
       "19                              [dumb]   \n",
       "20      [kill, lol the left is a joke]   \n",
       "21                                  []   \n",
       "22                              [fool]   \n",
       "23                          [ignorant]   \n",
       "24                                  []   \n",
       "\n",
       "                                        original_text  \\\n",
       "0   Another violent and aggressive immigrant killi...   \n",
       "1   I am 56 years old, I am not your fucking junio...   \n",
       "2                   Damn, a whole family. Sad indeed.   \n",
       "3   What a knucklehead. How can anyone not know th...   \n",
       "4   \"who do you think should do the killing?\"\\n\\nA...   \n",
       "5   But, but, but, is NOT a defense.  It's not eve...   \n",
       "6   Please people, stop using these silly, stupid ...   \n",
       "7                                               Dumb.   \n",
       "8   Obamacare is on it's last gasping breaths.   Y...   \n",
       "9           CROOKED Trump = GUILTY as hell.\\npathetic   \n",
       "10  You conveniently don't mention your cohorts wh...   \n",
       "11  Brilliant retort... It only makes sense that i...   \n",
       "12  Too bad.  Jeff Sessions has no common sense an...   \n",
       "13        This man is a sickening, mentally ill joke.   \n",
       "14  Religion is a crutch for the ignorant masses o...   \n",
       "15  No, I'm saying, based on previous comments you...   \n",
       "16  \"In doing so, they cite oil industry assurance...   \n",
       "17   What a sociopathic and parasitic leader we have.   \n",
       "18  We would all be better off if the President wa...   \n",
       "19                   Trump is so dumb!\\nHAHAHAHAHAHA!   \n",
       "20  Kill some more kids and then complain about gu...   \n",
       "21  Jim\\n\\nI think Donald is a con man and a dange...   \n",
       "22  There's plenty of wrong-wingers out there that...   \n",
       "23  Excellent point!  What if a mob assembled outs...   \n",
       "24  Exposing hypocrites like Trump and Pence is th...   \n",
       "\n",
       "                                            sentences  \\\n",
       "0   [another violent and aggressive immigrant kill...   \n",
       "1   [i am 56 years old, i am not your fucking juni...   \n",
       "2                [damn, a whole family., sad indeed.]   \n",
       "3   [what a knucklehead., how can anyone not know ...   \n",
       "4   [who do you think should do the killing?, anyo...   \n",
       "5   [but, but, but, is not a defense., its not eve...   \n",
       "6   [please people, stop using these silly, stupid...   \n",
       "7                                             [dumb.]   \n",
       "8   [obamacare is on its last gasping breaths., yo...   \n",
       "9         [crooked trump = guilty as hell., pathetic]   \n",
       "10  [you conveniently dont mention your cohorts wh...   \n",
       "11  [brilliant retort... it only makes sense that ...   \n",
       "12  [too bad., jeff sessions has no common sense a...   \n",
       "13      [this man is a sickening, mentally ill joke.]   \n",
       "14  [religion is a crutch for the ignorant masses ...   \n",
       "15  [no, im saying, based on previous comments you...   \n",
       "16  [in doing so, they cite oil industry assurance...   \n",
       "17  [what a sociopathic and parasitic leader we ha...   \n",
       "18  [we would all be better off if the president w...   \n",
       "19                 [trump is so dumb!, hahahahahaha!]   \n",
       "20  [kill some more kids and then complain about g...   \n",
       "21  [jim  i think donald is a con man and a danger...   \n",
       "22  [theres plenty of wrong-wingers out there that...   \n",
       "23  [excellent point!, what if a mob assembled out...   \n",
       "24  [exposing hypocrites like trump and pence is t...   \n",
       "\n",
       "                                    toxicity_sentence  \n",
       "0                                               [1.0]  \n",
       "1       [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]  \n",
       "2                                          [1.0, 0.0]  \n",
       "3                                     [1.0, 0.0, 0.0]  \n",
       "4   [1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, ...  \n",
       "5                           [0.0, 0.0, 0.0, 0.0, 0.0]  \n",
       "6                                               [1.0]  \n",
       "7                                               [1.0]  \n",
       "8                                [0.0, 1.0, 0.0, 0.0]  \n",
       "9                                          [0.0, 1.0]  \n",
       "10                                         [0.0, 1.0]  \n",
       "11                                         [0.0, 1.0]  \n",
       "12                                         [0.0, 1.0]  \n",
       "13                                              [1.0]  \n",
       "14                               [0.0, 0.0, 0.0, 1.0]  \n",
       "15                                    [0.0, 0.0, 1.0]  \n",
       "16                                              [1.0]  \n",
       "17                                              [1.0]  \n",
       "18                          [0.0, 0.0, 1.0, 0.0, 1.0]  \n",
       "19                                         [1.0, 0.0]  \n",
       "20                                              [1.0]  \n",
       "21                          [0.0, 0.0, 0.0, 0.0, 0.0]  \n",
       "22                                         [0.0, 1.0]  \n",
       "23                                    [0.0, 0.0, 1.0]  \n",
       "24                                         [0.0, 0.0]  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_preprocessed.head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = {\n",
    "    'sentence':  train_df_preprocessed.sentences.sum(),\n",
    "    'toxicity_sentence': train_df_preprocessed.toxicity_sentence.sum()\n",
    "        }\n",
    "\n",
    "train_df = pd.DataFrame (train_data, columns = ['sentence','toxicity_sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            sentence  toxicity_sentence\n",
      "0  another violent and aggressive immigrant killi...                1.0\n",
      "1   i am  years old i am not your fucking junior pal                1.0\n",
      "2                 what you are saying makes no sense                0.0\n",
      "3            i dont know what you are basing this on                0.0\n",
      "4  the cheap black market crap is still coming up...                0.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_df['sentence'] = train_df.apply(lambda row: preprocess_bayes(row.sentence), axis=1)\n",
    "print(train_df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13755"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df[train_df[\"toxicity_sentence\"] == 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9009"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df[train_df[\"toxicity_sentence\"] == 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming(text):\n",
    "    # Stem words\n",
    "    #data[i] = ' '.join([stemmer.stem(word) for word in data[i].split()])\n",
    "    data = ' '.join([lemmer.lemmatize(word) for word in text.split()])\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a grid search to find the best hyperparameters\n",
    "def grid_search(train):\n",
    "    # Create a pipeline\n",
    "    clf_pipeline = sklearn.pipeline.Pipeline([\n",
    "        ('v', CountVectorizer(strip_accents='ascii', stop_words='english')),\n",
    "        ('t', TfidfTransformer()), \n",
    "        ('c', sklearn.naive_bayes.MultinomialNB(fit_prior=True, class_prior=None))\n",
    "        ])\n",
    "    # Set parameters (name in pipeline + name of parameter)\n",
    "    parameters = { \n",
    "        'v__ngram_range': [(1, 1), (1, 2), (1, 3), (1, 4)], \n",
    "        'v__lowercase': (True, False), \n",
    "        't__use_idf': (True, False), \n",
    "        'c__alpha': (0.3, 0.6, 1.0) }\n",
    "    # Create a grid search classifier\n",
    "    gs_classifier = sklearn.model_selection.GridSearchCV(clf_pipeline, parameters, cv=5, iid=False, n_jobs=2, scoring='accuracy', verbose=1)\n",
    "    \n",
    "    # Start a search (Warning: takes a long time if the whole dataset is used)\n",
    "    # Slice: (train.data[:4000], train.target[:4000])\n",
    "    gs_classifier = gs_classifier.fit(train.data, train.target)\n",
    "    # Print results\n",
    "    print('---- Results ----')\n",
    "    print('Best score: ' + str(gs_classifier.best_score_))\n",
    "    for name in sorted(parameters.keys()):\n",
    "        print('{0}: {1}'.format(name, gs_classifier.best_params_[name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['sentence'] = train_df.apply(lambda row: stemming(row.sentence), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            sentence  toxicity_sentence\n",
      "0  another violent and aggressive immigrant killi...                1.0\n",
      "1     i am year old i am not your fucking junior pal                1.0\n",
      "2                  what you are saying make no sense                0.0\n",
      "3            i dont know what you are basing this on                0.0\n",
      "4  the cheap black market crap is still coming up...                0.0\n"
     ]
    }
   ],
   "source": [
    "print(train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate a model\n",
    "def train_and_evaluate(train):\n",
    "    \n",
    "    # Convert to bag of words\n",
    "    count_vect = CountVectorizer(strip_accents='ascii', stop_words='english', lowercase=True, ngram_range=(1,1))\n",
    "    X = count_vect.fit_transform(train['sentence'])\n",
    "    # Convert from occurrences to frequencies\n",
    "    # Occurrence count is a good start but there is an issue: longer documents will have higher average count values than shorter documents, even though they might talk about the same topics.\n",
    "    # To avoid these potential discrepancies it suffices to divide the number of occurrences of each word in a document by the total number of words in the document: these new features are called tf for Term Frequencies.\n",
    "    transformer = TfidfTransformer()\n",
    "    X = transformer.fit_transform(X)\n",
    "    # Create a model\n",
    "    # get the first vector out (for the first document) \n",
    "    first_vector_tfidfvectorizer=X[0] \n",
    " \n",
    "    # place tf-idf values in a pandas data frame \n",
    "    df = pd.DataFrame(first_vector_tfidfvectorizer.T.todense(), index=count_vect.get_feature_names(), columns=[\"tfidf\"]) \n",
    "    df.sort_values(by=[\"tfidf\"],ascending=True)\n",
    "    print(df.head)\n",
    "    model = sklearn.naive_bayes.MultinomialNB(alpha=0.3, fit_prior=True, class_prior=None)\n",
    "    # Train the model\n",
    "    model.fit(X, train['toxicity_sentence'])\n",
    "    # Save models\n",
    "    joblib.dump(count_vect, 'vectorizer.jbl')\n",
    "    joblib.dump(transformer, 'transformer.jbl')\n",
    "    joblib.dump(model, 'model.jbl')\n",
    "    # Evaluate on training data\n",
    "    print('-- Training data --')\n",
    "    predictions = model.predict(X)\n",
    "    print('here',model.coef_, len(count_vect.get_feature_names()), predictions)\n",
    "#     neg_class_prob_sorted = model.feature_log_prob_[0, :].argsort()\n",
    "#     pos_class_prob_sorted = model.feature_log_prob_[1, :].argsort()\n",
    "\n",
    "#     print(np.take(count_vect.get_feature_names(), neg_class_prob_sorted[:10]))\n",
    "#     print(np.take(count_vect.get_feature_names(), pos_class_prob_sorted[:10]))\n",
    "    print(predictions,X, train['sentence'])\n",
    "    accuracy = sklearn.metrics.accuracy_score(train['toxicity_sentence'], predictions)\n",
    "    print('Accuracy: {0:.2f}'.format(accuracy * 100.0))\n",
    "    print('Classification Report:')\n",
    "    print(sklearn.metrics.classification_report(train['toxicity_sentence'], predictions))\n",
    "    print('')\n",
    "#     Evaluate with 10-fold CV\n",
    "    print('-- 10-fold CV --')\n",
    "    predictions = sklearn.model_selection.cross_val_predict(model, X, train['toxicity_sentence'], cv=10)\n",
    "    accuracy = sklearn.metrics.accuracy_score(train['toxicity_sentence'], predictions)\n",
    "    print('Accuracy: {0:.2f}'.format(accuracy * 100.0))\n",
    "    print('Classification Report:')\n",
    "    print(sklearn.metrics.classification_report(train['toxicity_sentence'], predictions))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of          tfidf\n",
      "aa         0.0\n",
      "aaa        0.0\n",
      "aahhh      0.0\n",
      "aapoor     0.0\n",
      "abandon    0.0\n",
      "...        ...\n",
      "zuptoid    0.0\n",
      "zuri       0.0\n",
      "zwei       0.0\n",
      "zz         0.0\n",
      "zzzzzz     0.0\n",
      "\n",
      "[17812 rows x 1 columns]>\n",
      "-- Training data --\n",
      "here [[ -8.95407603 -10.42086336 -10.3399637  ... -11.38049662 -11.38049662\n",
      "  -11.38049662]] 17812 [1. 1. 0. ... 0. 0. 1.]\n",
      "[1. 1. 0. ... 0. 0. 1.]   (0, 17025)\t0.3612465882102366\n",
      "  (0, 13766)\t0.4250020107598058\n",
      "  (0, 8722)\t0.2989512824534744\n",
      "  (0, 8171)\t0.349476259432824\n",
      "  (0, 8079)\t0.3323369916456108\n",
      "  (0, 7760)\t0.3248117045584413\n",
      "  (0, 2604)\t0.3030879853045209\n",
      "  (0, 320)\t0.41177156518958624\n",
      "  (1, 17689)\t0.31672999210173375\n",
      "  (1, 11354)\t0.4934807414263961\n",
      "  (1, 10986)\t0.3800480289834355\n",
      "  (1, 8568)\t0.5424937095001565\n",
      "  (1, 6174)\t0.4662863291616598\n",
      "  (2, 14090)\t0.658914520992524\n",
      "  (2, 13808)\t0.5930647728437727\n",
      "  (2, 9514)\t0.46271571103319686\n",
      "  (3, 8797)\t0.3879256621830136\n",
      "  (3, 4524)\t0.35660724746419836\n",
      "  (3, 1290)\t0.8499087902097956\n",
      "  (4, 15236)\t0.28573483646110237\n",
      "  (4, 10447)\t0.30087565791668097\n",
      "  (4, 9904)\t0.34178024065434015\n",
      "  (4, 9634)\t0.29301245268805043\n",
      "  (4, 7543)\t0.42781878841233234\n",
      "  (4, 6526)\t0.2118224139209024\n",
      "  :\t:\n",
      "  (22758, 15484)\t0.5428257820100385\n",
      "  (22758, 12472)\t0.5215194238353845\n",
      "  (22758, 11514)\t0.34952743328602653\n",
      "  (22758, 3748)\t0.4316717564208178\n",
      "  (22759, 15484)\t0.4584978089563191\n",
      "  (22759, 12472)\t0.4405013930459622\n",
      "  (22759, 11514)\t0.29522835436877887\n",
      "  (22759, 6859)\t0.3526843100334865\n",
      "  (22759, 4806)\t0.5012491290437289\n",
      "  (22759, 3748)\t0.36461155874797907\n",
      "  (22760, 14130)\t0.46179909156333865\n",
      "  (22760, 12972)\t0.5908536207337577\n",
      "  (22760, 3748)\t0.483229289695829\n",
      "  (22760, 3320)\t0.451799791364658\n",
      "  (22761, 14815)\t1.0\n",
      "  (22762, 17493)\t0.44752665228896027\n",
      "  (22762, 12869)\t0.42008014164313046\n",
      "  (22762, 8921)\t0.7894634697616625\n",
      "  (22763, 17493)\t0.25461113215889375\n",
      "  (22763, 15053)\t0.3279680113578619\n",
      "  (22763, 13211)\t0.481421979708426\n",
      "  (22763, 11590)\t0.37421375480221036\n",
      "  (22763, 6365)\t0.4230934104454367\n",
      "  (22763, 3019)\t0.3817806264883548\n",
      "  (22763, 1680)\t0.36199809026153085 0        another violent and aggressive immigrant killi...\n",
      "1           i am year old i am not your fucking junior pal\n",
      "2                        what you are saying make no sense\n",
      "3                  i dont know what you are basing this on\n",
      "4        the cheap black market crap is still coming up...\n",
      "                               ...                        \n",
      "22759    i would earmark at least half the surplus to p...\n",
      "22760                         reduce the debt service cost\n",
      "22761                    who is this we of which you speak\n",
      "22762                          are you really a woman lars\n",
      "22763    if not then concern yourself with your penis a...\n",
      "Name: sentence, Length: 22764, dtype: object\n",
      "Accuracy: 87.89\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.87      0.94      0.90     13755\n",
      "         1.0       0.90      0.78      0.84      9009\n",
      "\n",
      "    accuracy                           0.88     22764\n",
      "   macro avg       0.88      0.86      0.87     22764\n",
      "weighted avg       0.88      0.88      0.88     22764\n",
      "\n",
      "\n",
      "-- 10-fold CV --\n",
      "Accuracy: 73.67\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.75      0.85      0.80     13755\n",
      "         1.0       0.71      0.56      0.63      9009\n",
      "\n",
      "    accuracy                           0.74     22764\n",
      "   macro avg       0.73      0.71      0.71     22764\n",
      "weighted avg       0.73      0.74      0.73     22764\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=0.3)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "train_and_evaluate(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "##load from file\n",
    "count_vect = joblib.load('vectorizer.jbl')\n",
    "transformer = joblib.load('transformer.jbl')\n",
    "model = joblib.load('model.jbl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas(Index=0, sentence='another violent and aggressive immigrant killing a innocent and intelligent u citizen sarcasm', toxicity_sentence=1.0)\n",
      "Pandas(Index=1, sentence='i am year old i am not your fucking junior pal', toxicity_sentence=1.0)\n",
      "Pandas(Index=2, sentence='what you are saying make no sense', toxicity_sentence=0.0)\n",
      "Pandas(Index=3, sentence='i dont know what you are basing this on', toxicity_sentence=0.0)\n",
      "Pandas(Index=4, sentence='the cheap black market crap is still coming up from mexican national and the good stuff from bc and humbolt', toxicity_sentence=0.0)\n",
      "Pandas(Index=5, sentence='the i corridor is still just a busy and they are still making a ton of money', toxicity_sentence=0.0)\n",
      "Pandas(Index=6, sentence='nothing ha really changed except that the legal market ha made it easy for anyone of legal age to buy it', toxicity_sentence=0.0)\n",
      "Pandas(Index=7, sentence='and the legal market is controlled by corporate grower with million dollar facility or a coop of smaller grower', toxicity_sentence=0.0)\n",
      "Pandas(Index=8, sentence='the federal government war on drug really ha no impact on the legal market in oregon', toxicity_sentence=0.0)\n",
      "Pandas(Index=9, sentence='i dont see any glutton of weed whatever that mean', toxicity_sentence=0.0)\n",
      "Pandas(Index=10, sentence='damn a whole family', toxicity_sentence=1.0)\n",
      "Pandas(Index=11, sentence='sad indeed', toxicity_sentence=0.0)\n",
      "Pandas(Index=12, sentence='what a knucklehead', toxicity_sentence=1.0)\n",
      "Pandas(Index=13, sentence='how can anyone not know this would be offensive', toxicity_sentence=0.0)\n",
      "Pandas(Index=14, sentence='', toxicity_sentence=0.0)\n",
      "Pandas(Index=15, sentence='who do you think should do the killing', toxicity_sentence=1.0)\n",
      "Pandas(Index=16, sentence='anyone and everyone', toxicity_sentence=0.0)\n",
      "Pandas(Index=17, sentence='this is a community problem so everyone who want to be part of the solution should be allowed to help', toxicity_sentence=0.0)\n",
      "Pandas(Index=18, sentence='a i said above remove all protection on free ranging cat and allow people to help', toxicity_sentence=0.0)\n"
     ]
    }
   ],
   "source": [
    "samples_to_test = train_df.head(19)\n",
    "for sample in samples_to_test.itertuples():\n",
    "    print(sample)\n",
    "    x = count_vect.transform([sample.sentence])\n",
    "    y = model.predict(x)\n",
    "    if y == 1.0:\n",
    "        toxic = getToxicWordsBayes(count_vect, x, 0.5)\n",
    "        spans = getSpansByToxicWords(toxic,sample.sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_semeval = SemEvalData()\n",
    "test_data_semeval.load_data(\"data/tsd_trial.csv\")\n",
    "test_df_preprocessed = test_data_semeval.preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_preprocessed[\"predicted_span\"]=[test_bayes(sentences[0])for sentences in test_df_preprocessed[\"sentences\"] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>spans</th>\n",
       "      <th>text</th>\n",
       "      <th>toxicity</th>\n",
       "      <th>toxic_words</th>\n",
       "      <th>original_text</th>\n",
       "      <th>sentences</th>\n",
       "      <th>toxicity_sentence</th>\n",
       "      <th>predicted_span</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[15, 16, 17, 18, 19, 27, 28, 29, 30, 31]</td>\n",
       "      <td>because hes a moron and a bigot. its not any m...</td>\n",
       "      <td>1</td>\n",
       "      <td>[moron, bigot]</td>\n",
       "      <td>Because he's a moron and a bigot. It's not any...</td>\n",
       "      <td>[because hes a moron and a bigot., its not any...</td>\n",
       "      <td>[1.0, 0.0]</td>\n",
       "      <td>[26, 27, 28, 29, 30, 14, 15, 16, 17, 18]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[29, 30, 31, 32, 33, 34]</td>\n",
       "      <td>how about we stop protecting idiots and let na...</td>\n",
       "      <td>1</td>\n",
       "      <td>[idiots]</td>\n",
       "      <td>How about we stop protecting idiots and let na...</td>\n",
       "      <td>[how about we stop protecting idiots and let n...</td>\n",
       "      <td>[1.0, 0.0]</td>\n",
       "      <td>[51, 52, 53, 60, 61, 62, 63, 64, 65, 74, 75, 7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[166, 167, 168, 169, 170, 171]</td>\n",
       "      <td>if people  were  smart, they would  boycott th...</td>\n",
       "      <td>1</td>\n",
       "      <td>[idiots]</td>\n",
       "      <td>If people  were  smart, they would  Boycott th...</td>\n",
       "      <td>[if people  were  smart, they would  boycott t...</td>\n",
       "      <td>[1.0]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[87, 88, 89, 90, 91, 92]</td>\n",
       "      <td>trump claimed that russia will never invade th...</td>\n",
       "      <td>1</td>\n",
       "      <td>[stupid]</td>\n",
       "      <td>Trump Claimed that Russia will never invade th...</td>\n",
       "      <td>[trump claimed that russia will never invade t...</td>\n",
       "      <td>[1.0]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[]</td>\n",
       "      <td>as long as your willing to pay a lot more for ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>As long as your willing to pay a lot more for ...</td>\n",
       "      <td>[as long as your willing to pay a lot more for...</td>\n",
       "      <td>[0.0, 0.0, 0.0]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>685</th>\n",
       "      <td>[129, 130, 131, 132, 133, 134]</td>\n",
       "      <td>but ... trumps not bluffing. hes prepared to g...</td>\n",
       "      <td>1</td>\n",
       "      <td>[stupid]</td>\n",
       "      <td>But ... Trump's not bluffing. He's prepared to...</td>\n",
       "      <td>[but ... trumps not bluffing., hes prepared to...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0, 0.0]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>686</th>\n",
       "      <td>[126, 127, 128, 129, 130, 131]</td>\n",
       "      <td>cant believe the limited knowledge of this art...</td>\n",
       "      <td>1</td>\n",
       "      <td>[stupid]</td>\n",
       "      <td>Can't believe the limited knowledge of this Ar...</td>\n",
       "      <td>[cant believe the limited knowledge of this ar...</td>\n",
       "      <td>[0.0, 1.0]</td>\n",
       "      <td>[43, 44, 45, 46, 47, 48, 49, 5, 6, 7, 8, 9, 10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>687</th>\n",
       "      <td>[24, 25, 26, 27, 28, 29]</td>\n",
       "      <td>i think it conservative idiots who cannot reac...</td>\n",
       "      <td>1</td>\n",
       "      <td>[idiots]</td>\n",
       "      <td>I think it conservative idiots who cannot reac...</td>\n",
       "      <td>[i think it conservative idiots who cannot rea...</td>\n",
       "      <td>[1.0]</td>\n",
       "      <td>[11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>688</th>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...</td>\n",
       "      <td>youre an id*ot...go away.</td>\n",
       "      <td>1</td>\n",
       "      <td>[youre an id*ot]</td>\n",
       "      <td>You're an id*ot...Go away.</td>\n",
       "      <td>[youre an id*ot...go away.]</td>\n",
       "      <td>[1.0]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>689</th>\n",
       "      <td>[136, 137, 138, 139, 140, 141]</td>\n",
       "      <td>unless there is wording in the employment cont...</td>\n",
       "      <td>1</td>\n",
       "      <td>[stupid]</td>\n",
       "      <td>Unless there is wording in the employment cont...</td>\n",
       "      <td>[unless there is wording in the employment con...</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>[93, 94, 95, 96, 97, 98, 99, 141, 142, 143, 14...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>690 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 spans  \\\n",
       "0             [15, 16, 17, 18, 19, 27, 28, 29, 30, 31]   \n",
       "1                             [29, 30, 31, 32, 33, 34]   \n",
       "2                       [166, 167, 168, 169, 170, 171]   \n",
       "3                             [87, 88, 89, 90, 91, 92]   \n",
       "4                                                   []   \n",
       "..                                                 ...   \n",
       "685                     [129, 130, 131, 132, 133, 134]   \n",
       "686                     [126, 127, 128, 129, 130, 131]   \n",
       "687                           [24, 25, 26, 27, 28, 29]   \n",
       "688  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...   \n",
       "689                     [136, 137, 138, 139, 140, 141]   \n",
       "\n",
       "                                                  text  toxicity  \\\n",
       "0    because hes a moron and a bigot. its not any m...         1   \n",
       "1    how about we stop protecting idiots and let na...         1   \n",
       "2    if people  were  smart, they would  boycott th...         1   \n",
       "3    trump claimed that russia will never invade th...         1   \n",
       "4    as long as your willing to pay a lot more for ...         0   \n",
       "..                                                 ...       ...   \n",
       "685  but ... trumps not bluffing. hes prepared to g...         1   \n",
       "686  cant believe the limited knowledge of this art...         1   \n",
       "687  i think it conservative idiots who cannot reac...         1   \n",
       "688                          youre an id*ot...go away.         1   \n",
       "689  unless there is wording in the employment cont...         1   \n",
       "\n",
       "          toxic_words                                      original_text  \\\n",
       "0      [moron, bigot]  Because he's a moron and a bigot. It's not any...   \n",
       "1            [idiots]  How about we stop protecting idiots and let na...   \n",
       "2            [idiots]  If people  were  smart, they would  Boycott th...   \n",
       "3            [stupid]  Trump Claimed that Russia will never invade th...   \n",
       "4                  []  As long as your willing to pay a lot more for ...   \n",
       "..                ...                                                ...   \n",
       "685          [stupid]  But ... Trump's not bluffing. He's prepared to...   \n",
       "686          [stupid]  Can't believe the limited knowledge of this Ar...   \n",
       "687          [idiots]  I think it conservative idiots who cannot reac...   \n",
       "688  [youre an id*ot]                         You're an id*ot...Go away.   \n",
       "689          [stupid]  Unless there is wording in the employment cont...   \n",
       "\n",
       "                                             sentences  \\\n",
       "0    [because hes a moron and a bigot., its not any...   \n",
       "1    [how about we stop protecting idiots and let n...   \n",
       "2    [if people  were  smart, they would  boycott t...   \n",
       "3    [trump claimed that russia will never invade t...   \n",
       "4    [as long as your willing to pay a lot more for...   \n",
       "..                                                 ...   \n",
       "685  [but ... trumps not bluffing., hes prepared to...   \n",
       "686  [cant believe the limited knowledge of this ar...   \n",
       "687  [i think it conservative idiots who cannot rea...   \n",
       "688                        [youre an id*ot...go away.]   \n",
       "689  [unless there is wording in the employment con...   \n",
       "\n",
       "             toxicity_sentence  \\\n",
       "0                   [1.0, 0.0]   \n",
       "1                   [1.0, 0.0]   \n",
       "2                        [1.0]   \n",
       "3                        [1.0]   \n",
       "4              [0.0, 0.0, 0.0]   \n",
       "..                         ...   \n",
       "685  [0.0, 0.0, 0.0, 1.0, 0.0]   \n",
       "686                 [0.0, 1.0]   \n",
       "687                      [1.0]   \n",
       "688                      [1.0]   \n",
       "689       [1.0, 0.0, 0.0, 0.0]   \n",
       "\n",
       "                                        predicted_span  \n",
       "0             [26, 27, 28, 29, 30, 14, 15, 16, 17, 18]  \n",
       "1    [51, 52, 53, 60, 61, 62, 63, 64, 65, 74, 75, 7...  \n",
       "2                                                   []  \n",
       "3                                                   []  \n",
       "4                                                   []  \n",
       "..                                                 ...  \n",
       "685                                                 []  \n",
       "686  [43, 44, 45, 46, 47, 48, 49, 5, 6, 7, 8, 9, 10...  \n",
       "687  [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 2...  \n",
       "688                                                 []  \n",
       "689  [93, 94, 95, 96, 97, 98, 99, 141, 142, 143, 14...  \n",
       "\n",
       "[690 rows x 8 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df_preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_preprocessed[\"Pscore\"] = [ 1 if (len(s) == 0 and len(ps) == 0) \n",
    "                             else 0 if len(ps) == 0 \n",
    "                             else len( set(s).intersection(set(ps) ))/ len(set(ps))  for s, ps in zip(test_df_preprocessed[\"spans\"],test_df_preprocessed[\"predicted_span\"]) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_preprocessed[\"Rscore\"] = [ 1 if (len(s) == 0 and len(ps) == 0) \n",
    "                             else 0 if len(s) == 0 \n",
    "                             else len( set(s).intersection(set(ps) ))/ len(set(s))  for s, ps in zip(test_df_preprocessed[\"spans\"],test_df_preprocessed[\"predicted_span\"]) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_preprocessed[\"Fscore\"] = 2 * test_df_preprocessed[\"Pscore\"] *test_df_preprocessed[\"Rscore\"] /(test_df_preprocessed[\"Pscore\"] + test_df_preprocessed[\"Rscore\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "F_score= np.mean(test_df_preprocessed[\"Fscore\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.47098965465227693"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
